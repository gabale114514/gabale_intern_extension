"""
çƒ­æœè¯é¢˜å¤„ç†å·¥å…·æ¥å£
"""

import logging
import json
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
import hashlib

from config.platform_config import TOOL_CONFIG, PROCESSING_CONFIG
from main.scraper.utils import (
    clean_text, extract_tags, generate_hash, categorize_topic,
    validate_platform, validate_rank, validate_heat_value,
    is_duplicate_topic, get_platform_icon, get_platform_name
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('hot_topic_tool.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class HotTopicItem:
    """çƒ­æœè¯é¢˜æ•°æ®æ¨¡å‹"""
    platform: str
    title: str
    rank: int
    tags: List[str] = None
    url: Optional[str] = None
    heat_value: Optional[int] = None
    category: Optional[str] = None
    created_at: Optional[datetime] = None
    hash_id: Optional[str] = None
    
    def __post_init__(self):
        if self.tags is None:
            self.tags = []
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.hash_id is None:
            self.hash_id = self._generate_hash()
    
    def _generate_hash(self) -> str:
        """ç”Ÿæˆç”¨äºå»é‡çš„å“ˆå¸Œå€¼"""
        content = f"{self.platform}_{self.title}_{self.rank}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()

class HotTopicProcessor:
    """çƒ­æœè¯é¢˜å¤„ç†å™¨"""
    
    def __init__(self):
        self.processed_items = []
        self.duplicate_count = 0
        self.error_count = 0
        
    def process_single_topic(self, raw_data: Dict[str, Any]) -> Optional[HotTopicItem]:
        """
        å¤„ç†å•ä¸ªçƒ­æœè¯é¢˜
        
        Args:
            raw_data: åŸå§‹æ•°æ®å­—å…¸
            
        Returns:
            å¤„ç†åçš„HotTopicItemå¯¹è±¡ï¼Œå¦‚æœå¤„ç†å¤±è´¥è¿”å›None
        """
        try:
            # éªŒè¯å¿…éœ€å­—æ®µ
            if not self._validate_required_fields(raw_data):
                logger.error(f"å¿…éœ€å­—æ®µéªŒè¯å¤±è´¥: {raw_data}")
                self.error_count += 1
                return None
            
            # æ¸…ç†å’ŒéªŒè¯æ•°æ®
            platform = raw_data['platform']
            title = clean_text(raw_data['title'])
            rank = raw_data['rank']
            
            # éªŒè¯å¹³å°
            if not validate_platform(platform):
                logger.error(f"æ— æ•ˆçš„å¹³å°: {platform}")
                self.error_count += 1
                return None
            
            # éªŒè¯æ’å
            if not validate_rank(rank):
                logger.error(f"æ— æ•ˆçš„æ’å: {rank}")
                self.error_count += 1
                return None
            
            # éªŒè¯çƒ­åº¦å€¼
            heat_value = raw_data.get('heat_value')
            if not validate_heat_value(heat_value):
                logger.error(f"æ— æ•ˆçš„çƒ­åº¦å€¼: {heat_value}")
                self.error_count += 1
                return None
            
            # æå–æ ‡ç­¾
            tags = extract_tags(title)
            
            # è‡ªåŠ¨åˆ†ç±»
            category = categorize_topic(title)
            
            # åˆ›å»ºHotTopicItemå¯¹è±¡
            item = HotTopicItem(
                platform=platform,
                title=title,
                rank=rank,
                tags=tags,
                url=raw_data.get('url'),
                heat_value=heat_value,
                category=category
            )
            
            # æ£€æŸ¥é‡å¤
            if self._is_duplicate_with_existing(item):
                self.duplicate_count += 1
                return None
            
            # æ·»åŠ åˆ°å·²å¤„ç†åˆ—è¡¨
            self.processed_items.append(item)
            
            logger.info(f"æˆåŠŸå¤„ç†è¯é¢˜: {title} (å¹³å°: {platform}, æ’å: {rank})")
            return item
            
        except Exception as e:
            logger.error(f"å¤„ç†è¯é¢˜æ—¶å‘ç”Ÿé”™è¯¯: {e}, æ•°æ®: {raw_data}")
            self.error_count += 1
            return None
    
    def process_batch_topics(self, raw_data_list: List[Dict[str, Any]]) -> List[HotTopicItem]:
        """
        æ‰¹é‡å¤„ç†çƒ­æœè¯é¢˜
        
        Args:
            raw_data_list: åŸå§‹æ•°æ®åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„HotTopicItemå¯¹è±¡åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(raw_data_list)} æ¡æ•°æ®")
        
        processed_items = []
        for raw_data in raw_data_list:
            item = self.process_single_topic(raw_data)
            if item:
                processed_items.append(item)
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ {len(processed_items)} æ¡, "
                   f"é‡å¤ {self.duplicate_count} æ¡, é”™è¯¯ {self.error_count} æ¡")
        
        return processed_items
    
    def _validate_required_fields(self, data: Dict[str, Any]) -> bool:
        """éªŒè¯å¿…éœ€å­—æ®µ"""
        required_fields = ['platform', 'title', 'rank']
        return all(field in data and data[field] for field in required_fields)
    
    def _is_duplicate_with_existing(self, new_item: HotTopicItem) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸å·²å¤„ç†çš„é¡¹ç›®é‡å¤"""
        for existing_item in self.processed_items:
            if is_duplicate_topic(
                {'title': new_item.title, 'platform': new_item.platform},
                {'title': existing_item.title, 'platform': existing_item.platform}
            ):
                return True
        return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_processed': len(self.processed_items),
            'duplicate_count': self.duplicate_count,
            'error_count': self.error_count,
            'success_rate': len(self.processed_items) / (len(self.processed_items) + self.error_count) * 100 if (len(self.processed_items) + self.error_count) > 0 else 0
        }
    
    def export_to_dict(self) -> List[Dict[str, Any]]:
        """å¯¼å‡ºä¸ºå­—å…¸æ ¼å¼"""
        result = []
        for item in self.processed_items:
            item_dict = asdict(item)
            # å¤„ç†datetimeåºåˆ—åŒ–
            if item_dict['created_at']:
                item_dict['created_at'] = item_dict['created_at'].isoformat()
            result.append(item_dict)
        return result
    
    def export_to_json(self, filepath: str = None) -> str:
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        data = self.export_to_dict()
        json_str = json.dumps(data, ensure_ascii=False, indent=2, default=str)
        
        if filepath:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(json_str)
            logger.info(f"æ•°æ®å·²å¯¼å‡ºåˆ°: {filepath}")
        
        return json_str

# å·¥å…·æ¥å£å‡½æ•°
def process_hot_topic(raw_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    å¤„ç†å•ä¸ªçƒ­æœè¯é¢˜çš„å·¥å…·æ¥å£
    
    Args:
        raw_data: åŸå§‹æ•°æ®å­—å…¸
        
    Returns:
        å¤„ç†åçš„æ•°æ®å­—å…¸ï¼Œå¦‚æœå¤„ç†å¤±è´¥è¿”å›None
    """
    processor = HotTopicProcessor()
    item = processor.process_single_topic(raw_data)
    if item:
        item_dict = asdict(item)
        # å¤„ç†datetimeåºåˆ—åŒ–
        if item_dict['created_at']:
            item_dict['created_at'] = item_dict['created_at'].isoformat()
        return item_dict
    return None

def process_hot_topics_batch(raw_data_list: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    æ‰¹é‡å¤„ç†çƒ­æœè¯é¢˜çš„å·¥å…·æ¥å£
    
    Args:
        raw_data_list: åŸå§‹æ•°æ®åˆ—è¡¨
        
    Returns:
        åŒ…å«å¤„ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    processor = HotTopicProcessor()
    processed_items = processor.process_batch_topics(raw_data_list)
    
    return {
        'success': True,
        'data': processor.export_to_dict(),
        'statistics': processor.get_statistics(),
        'timestamp': datetime.now().isoformat()
    }

def get_tool_info() -> Dict[str, Any]:
    """
    è·å–å·¥å…·ä¿¡æ¯
    
    Returns:
        å·¥å…·ä¿¡æ¯å­—å…¸
    """
    return {
        **TOOL_CONFIG,
        'supported_platforms': list(PROCESSING_CONFIG.keys()),
        'features': {
            'auto_categorize': PROCESSING_CONFIG['enable_auto_categorize'],
            'duplicate_check': PROCESSING_CONFIG['enable_duplicate_check'],
            'max_title_length': PROCESSING_CONFIG['max_title_length'],
            'max_tags_count': PROCESSING_CONFIG['max_tags_count']
        }
    }

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    # ç¤ºä¾‹æ•°æ®
    sample_data = [
        {
            'platform': 'weibo',
            'title': 'è¿™æ˜¯ä¸€ä¸ªçƒ­é—¨è¯é¢˜ğŸ”¥ æ–°æ¶ˆæ¯',
            'rank': 1,
            'heat_value': 10000,
            'url': 'https://example.com'
        },
        {
            'platform': 'zhihu',
            'title': 'AIæŠ€æœ¯æœ€æ–°çªç ´',
            'rank': 2,
            'heat_value': 8000
        }
    ]
    
    # æµ‹è¯•æ‰¹é‡å¤„ç†
    result = process_hot_topics_batch(sample_data)
    print(json.dumps(result, ensure_ascii=False, indent=2))
