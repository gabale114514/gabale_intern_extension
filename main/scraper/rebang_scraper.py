import logging
import requests
import time
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from main.scraper.api_fetcher import ApiFetcher
from main.scraper.data_parser import DataParser
from main.scraper.deduplicator import Deduplicator
from main.scraper.storage_manager import StorageManager
from config.database_config import DATABASE_CONFIG
from main.database.database_manager import get_db_manager              
from config.platform_config import PLATFORM_CONFIG, platform_categories, custom_params
from main.scraper.utils import setup_logging
setup_logging()
logger = logging.getLogger(__name__)

class RebangScraper:
    def __init__(self):
        self.session = requests.Session()
        self.base_url = "https://rebang.today"
        self.api_fetcher = ApiFetcher(self.session, self.base_url)
        self.data_parser = DataParser()
        self.deduplicator = Deduplicator()
        self.storage_manager = StorageManager()
        self.platform_config = PLATFORM_CONFIG
    def should_stop_pagination(self, current_page: int, current_topics: List, config: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢ç¿»é¡µ"""
        pagination = config.get('pagination', {})
        
        # è¾¾åˆ°æœ€å¤§é¡µæ•°
        if current_page >= pagination.get('max_pages', 5):
            return True
            
        # å½“å‰é¡µæ— æ•°æ®
        if not current_topics:
            return True
            
        # å½“å‰é¡µæ•°æ®é‡ä¸è¶³(å¯èƒ½æ²¡æœ‰ä¸‹ä¸€é¡µ)
        if len(current_topics) < pagination.get('page_size', 20):
            return True
            
        return False
    def scrape_platform_category(self, platform_code: str, category: str, extra_params: Optional[Dict] = None) -> Tuple[List[Dict], Dict[str, int]]:
        """çˆ¬å–å•ä¸ªå¹³å°çš„å•ä¸ªåˆ†ç±»(æ”¯æŒå¤šé¡µå’Œrankè°ƒæ•´)"""
        config = self.platform_config.get(platform_code)
        if not config:
            return [], {'total_count': 0, 'success_count': 0, 'error_count': 0, 'duplicate_count': 0}
        
        pagination = config.get('pagination', {'max_pages': 1})
        max_pages = pagination.get('max_pages', 1)
        page_param = pagination.get('param_name', 'page')
        page_size = pagination.get('page_size', 20)  # é»˜è®¤æ¯é¡µ20æ¡
        start_page = pagination.get('start_page', 1)  # æ˜ç¡®è·å–èµ·å§‹é¡µç 
        
        all_topics = []
        total_stats = {'total_count': 0, 'success_count': 0, 'error_count': 0, 'duplicate_count': 0}
        
        page = start_page  # ä»é…ç½®çš„èµ·å§‹é¡µç å¼€å§‹
        while True:
            # åˆå¹¶å‚æ•°
            params = config['default_params'].copy()
            params['sub_tab'] = category
            params[page_param] = str(page)
            if extra_params:
                params.update(extra_params)
            params['t'] = int(time.time() * 1000)
            try:
                params['page']=page
                # è·å–æ•°æ®
                api_data = self.api_fetcher.fetch_data(config['base_url'], params)
                if not api_data:
                    logger.warning(f"å¹³å° {platform_code} åˆ†ç±» {category} ç¬¬ {page} é¡µæ— æ•°æ®")
                    break
                    
                # è§£ææ•°æ®
                topics = self.data_parser.parse_api_data(api_data, platform_code, category, config, page)
                if not topics:
                    logger.info(f"å¹³å° {platform_code} åˆ†ç±» {category} ç¬¬ {page} é¡µæ— æœ‰æ•ˆæ•°æ®")
                    break
                for topic in topics:
                    topic['rank'] += (page - 1) * page_size
                # å­˜å‚¨æ•°æ®
                current_hashes = [t['hash_id'] for t in topics]
                self.storage_manager.mark_inactive_by_category(platform_code, current_hashes, category)
                save_stats = self.storage_manager.save_topics(topics, self.deduplicator)
                
                # æ›´æ–°ç»Ÿè®¡
                all_topics.extend(topics)
                total_stats['total_count'] += save_stats['total_count']
                total_stats['success_count'] += save_stats['success_count']
                total_stats['error_count'] += save_stats['error_count']
                total_stats['duplicate_count'] += save_stats['duplicate_count']
                
                logger.info(f"å¹³å° {platform_code} åˆ†ç±» {category} ç¬¬ {page} é¡µå®Œæˆ: æ–°å¢ {save_stats['success_count']} æ¡ (æ’åè°ƒæ•´: +{(page-1)*page_size})")
                
                # åœæ­¢æ¡ä»¶åˆ¤æ–­
                if self.should_stop_pagination(page, topics, config):
                    break
                    
                page += 1  # é€’å¢é¡µç 
                
            except Exception as e:
                logger.error(f"å¹³å° {platform_code} åˆ†ç±» {category} ç¬¬ {page} é¡µå¼‚å¸¸: {e}")
                break
        
        return all_topics, total_stats
    
    def scrape_platform(self, platform_code: str, categories: List[str], extra_params: Optional[Dict] = None) -> Dict[str, Dict]:
        """
        çˆ¬å–å•ä¸ªå¹³å°çš„å¤šä¸ªåˆ†ç±»
        :param platform_code: å¹³å°ä»£ç 
        :param categories: åˆ†ç±»åˆ—è¡¨
        :param extra_params: é¢å¤–å‚æ•°
        :return: å„åˆ†ç±»çš„çˆ¬å–ç»“æœ
        """
        category_results = {}
        for category in categories:
            try:
                start_time = datetime.now()
                topics, stats = self.scrape_platform_category(platform_code, category, extra_params)
                end_time = datetime.now()

                status = 'success' if stats['success_count'] == stats['total_count'] else \
                        'partial' if stats['success_count'] > 0 else 'failed'

                # ä¿å­˜æ—¥å¿—
                self.storage_manager.save_collection_log(
                    platform=platform_code,
                    category=category,
                    status=status,
                    stats=stats,
                    start_time=start_time.isoformat(),
                    end_time=end_time.isoformat()
                )

                category_results[category] = {
                    'status': status,
                    'stats': stats,
                    'duration': (end_time - start_time).total_seconds()
                }
                logger.info(f"å¹³å° {platform_code} åˆ†ç±» {category} å®Œæˆ: æˆåŠŸ {stats['success_count']} æ¡")
            except Exception as e:
                logger.error(f"å¹³å° {platform_code} åˆ†ç±» {category} å¼‚å¸¸: {e}")
                category_results[category] = {'status': 'error', 'error': str(e)}
        return category_results
    
    def scrape_all_platforms(self, platform_categories: Dict[str, List[str]], platform_extra_params: Optional[Dict[str, Dict]] = None) -> Dict[str, Dict]:
        """
        çˆ¬å–æ‰€æœ‰å¹³å°çš„å¤šä¸ªåˆ†ç±»
        :param platform_categories: å¹³å°-åˆ†ç±»æ˜ å°„
        :param platform_extra_params: å„å¹³å°çš„é¢å¤–å‚æ•°
        :return: å„å¹³å°çš„çˆ¬å–ç»“æœ
        """
        results = {}
        platform_extra_params = platform_extra_params or {}
        
        enabled_platforms = self.deduplicator.db.get_enabled_platforms()
        enabled_codes = {p['code'] for p in enabled_platforms}
        
        for platform_code, categories in platform_categories.items():
            if platform_code not in enabled_codes:
                logger.info(f"å¹³å° {platform_code} å·²ç¦ç”¨ï¼Œè·³è¿‡æ‰€æœ‰åˆ†ç±»")
                continue
                
            try:
                extra_params = platform_extra_params.get(platform_code, {})
                platform_result = self.scrape_platform(platform_code, categories, extra_params)
                results[platform_code] = platform_result
            except Exception as e:
                logger.error(f"å¹³å° {platform_code} æ•´ä½“å¼‚å¸¸: {e}")
                results[platform_code] = {'status': 'error', 'error': str(e)}
                
        return results

_scraper_instance = None

def get_scraper() -> RebangScraper:
    global _scraper_instance
    if not _scraper_instance:
        _scraper_instance = RebangScraper()
    return _scraper_instance

def run_scheduled_scraping(platform_categories: Dict[str, List[str]], 
                        platform_extra_params: Optional[Dict[str, Dict]] = None):
    logger.info("å¼€å§‹å®šæ—¶é‡‡é›†æ‰€æœ‰å¹³å°")
    scraper = get_scraper()
    # ä¼ å…¥platform_categorieså‚æ•°
    results = scraper.scrape_all_platforms(
        platform_categories=platform_categories,
        platform_extra_params=platform_extra_params
    )
    total_success = 0
    for platform_results in results.values():
        if isinstance(platform_results, dict):
            for category_result in platform_results.values():
                if 'stats' in category_result:
                    total_success += category_result['stats']['success_count']
    logger.info(f"å®šæ—¶é‡‡é›†å®Œæˆï¼Œæ€»æˆåŠŸæ•°: {total_success}")
    return results
if __name__ == "__main__":
    # åˆå§‹åŒ–æ‰“å°
    print(f"\n{'='*60}")
    print(f"çƒ­æ¦œä»Šæ—¥çˆ¬è™« - å¼€å§‹é‡‡é›† ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
    print(f"{'='*60}")
    
    db = get_db_manager()
    if not db.connect():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥")
        exit(1)
    try:
        scraper = RebangScraper()
        start_time = time.time()
        
        # æ‰§è¡Œçˆ¬å–
        results = scraper.scrape_all_platforms(
            platform_categories=platform_categories,
            platform_extra_params=custom_params
        )
        # ç»“æœç»Ÿè®¡
        total_stats = {
            'platforms': 0,
            'categories': 0,
            'success': 0,
            'duplicate': 0,
            'error': 0,
            'disabled': 0
        }
        # æ‰“å°è¯¦ç»†ç»“æœ
        print("\nğŸ“Š é‡‡é›†ç»“æœè¯¦æƒ…:")
        for platform, category_results in results.items():
            total_stats['platforms'] += 1
            print(f"\nğŸ”¹ å¹³å° [{platform.upper()}]")
            
            for category, res in category_results.items():
                total_stats['categories'] += 1
                
                if 'stats' in res:
                    status_icon = "âœ…" if res['status'] == 'success' else \
                                 "âš ï¸" if res['status'] == 'partial' else "âŒ"
                    
                    print(f"   â”œâ”€ {status_icon} åˆ†ç±» {category}:")
                    print(f"   â”‚   â”œâ”€ çŠ¶æ€: {res['status']}")
                    print(f"   â”‚   â”œâ”€ æˆåŠŸ: {res['stats']['success_count']}")
                    print(f"   â”‚   â”œâ”€ é‡å¤: {res['stats']['duplicate_count']}")
                    print(f"   â”‚   â””â”€ é”™è¯¯: {res['stats']['error_count']}")
                    
                    total_stats['success'] += res['stats']['success_count']
                    total_stats['duplicate'] += res['stats']['duplicate_count']
                    total_stats['error'] += res['stats']['error_count']
                else:
                    print(f"   â””â”€ â— åˆ†ç±» {category} é‡‡é›†å¤±è´¥: {res.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    total_stats['error'] += 1

        # æ‰“å°æ€»ç»“æŠ¥å‘Š
        duration = time.time() - start_time
        print(f"\n{'='*60}")
        print("ğŸ“ˆ é‡‡é›†æ€»ç»“æŠ¥å‘Š")
        print(f"{'-'*60}")
        print(f"ğŸ•’ è€—æ—¶: {duration:.2f}ç§’")
        print(f"ğŸ æ€»å¹³å°æ•°: {total_stats['platforms']}")
        print(f"ğŸ“‚ æ€»åˆ†ç±»æ•°: {total_stats['categories']}")
        print(f"ğŸŸ¢ æˆåŠŸæ€»æ•°: {total_stats['success']}")
        print(f"ğŸŸ¡ é‡å¤æ€»æ•°: {total_stats['duplicate']}")
        print(f"ğŸ”´ é”™è¯¯æ€»æ•°: {total_stats['error']}")
        print(f"ğŸš« ç¦ç”¨å¹³å°: {total_stats['disabled']}")
        print(f"{'='*60}\n")
        
    except Exception as e:
        print(f"\nâ€¼ï¸ é‡‡é›†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        db.disconnect()