"""
å·¥å…·å‡½æ•°æ¨¡å— - æ•°æ®å¤„ç†å’Œæ¸…æ´—
"""

import re
import hashlib
import json
from typing import List, Dict, Any, Optional
from datetime import datetime
import logging
from config.platform_config import TAG_PATTERNS, CATEGORY_KEYWORDS, PROCESSING_CONFIG

logger = logging.getLogger(__name__)

def clean_text(text: Any) -> str:
    """
    æ¸…ç†æ–‡æœ¬å†…å®¹ï¼ˆå¢å¼ºç‰ˆï¼‰
    åŠŸèƒ½ï¼š
    1. æ”¯æŒå¤„ç†å­—ç¬¦ä¸²ã€æ•°å€¼å’ŒNoneç±»å‹è¾“å…¥
    2. ç§»é™¤å¤šä½™ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦
    3. è‡ªåŠ¨è¿›è¡Œç±»å‹è½¬æ¢å’Œé•¿åº¦é™åˆ¶
    
    å‚æ•°ï¼š
    text: å¯ä»¥æ˜¯str/int/float/Noneç­‰ç±»å‹
    
    è¿”å›ï¼š
    å¤„ç†åçš„å­—ç¬¦ä¸²
    """
    # å¤„ç†ç©ºå€¼å’Œç±»å‹è½¬æ¢
    if text is None:
        return ""
    
    # æ•°å€¼ç±»å‹å¤„ç†
    if isinstance(text, (int, float)):
        text = str(text)
    
    # ç¡®ä¿ç°åœ¨æ˜¯å­—ç¬¦ä¸²ç±»å‹
    if not isinstance(text, str):
        return ""
    
    # åŸå§‹æ¸…ç†é€»è¾‘
    text = text.strip()
    text = re.sub(r'\s+', ' ', text)  # åˆå¹¶è¿ç»­ç©ºç™½
    text = re.sub(r'[^\w\s\u4e00-\u9fff\-\.\,\!\?\(\)\[\]ã€ã€‘]', '', text)  # è¿‡æ»¤ç‰¹æ®Šå­—ç¬¦
    
    # é•¿åº¦é™åˆ¶
    max_length = PROCESSING_CONFIG.get('max_title_length', 255)  # é»˜è®¤å€¼é˜²æ­¢KeyError
    if len(text) > max_length:
        text = text[:max_length] + "..."
    
    return text

def extract_tags(text: str) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–æ ‡ç­¾
    """
    tags = []
    for tag_name, pattern in TAG_PATTERNS.items():
        if re.search(pattern, text, re.IGNORECASE):
            tags.append(tag_name)
    
    # é™åˆ¶æ ‡ç­¾æ•°é‡
    max_tags = PROCESSING_CONFIG['max_tags_count']
    return tags[:max_tags]

def generate_hash(content: str) -> str:
    """
    ç”Ÿæˆå†…å®¹å“ˆå¸Œå€¼
    """
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def categorize_topic(title: str) -> str:
    """
    å¯¹è¯é¢˜è¿›è¡Œåˆ†ç±»
    """
    if not PROCESSING_CONFIG['enable_auto_categorize']:
        return 'å…¶ä»–'
    
    for category, keywords in CATEGORY_KEYWORDS.items():
        for keyword in keywords:
            if keyword in title:
                return category
    
    return 'å…¶ä»–'

def calculate_similarity(text1: str, text2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„Jaccardç›¸ä¼¼åº¦ï¼‰
    """
    if not text1 or not text2:
        return 0.0
    
    # åˆ†è¯ï¼ˆç®€å•çš„æŒ‰å­—ç¬¦åˆ†å‰²ï¼‰
    set1 = set(text1)
    set2 = set(text2)
    
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    
    return intersection / union if union > 0 else 0.0

def is_duplicate_topic(topic1: Dict[str, Any], topic2: Dict[str, Any]) -> bool:
    """
    åˆ¤æ–­ä¸¤ä¸ªè¯é¢˜æ˜¯å¦é‡å¤
    """
    if not PROCESSING_CONFIG['enable_duplicate_check']:
        return False
    
    title1 = topic1.get('title', '')
    title2 = topic2.get('title', '')
    
    if not title1 or not title2:
        return False
    
    similarity = calculate_similarity(title1, title2)
    threshold = PROCESSING_CONFIG['similarity_threshold']
    
    return similarity >= threshold

def validate_platform(platform: str) -> bool:
    """
    éªŒè¯å¹³å°æ˜¯å¦æœ‰æ•ˆ
    """
    from config.platform_config import PLATFORM_CONFIG
    return platform in PLATFORM_CONFIG and PLATFORM_CONFIG[platform]['enabled']

def validate_rank(rank: int) -> bool:
    """
    éªŒè¯æ’åæ˜¯å¦æœ‰æ•ˆ
    """
    return isinstance(rank, int) and 1 <= rank <= 1000

def validate_heat_value(heat_value: Optional[int]) -> bool:
    """
    éªŒè¯çƒ­åº¦å€¼æ˜¯å¦æœ‰æ•ˆ
    """
    if heat_value is None:
        return True
    return isinstance(heat_value, int) and heat_value >= 0

def format_datetime(dt: datetime) -> str:
    """
    æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´
    """
    return dt.strftime('%Y-%m-%d %H:%M:%S')

def safe_json_loads(data: str, default: Any = None) -> Any:
    """
    å®‰å…¨çš„JSONè§£æ
    """
    try:
        return json.loads(data)
    except (json.JSONDecodeError, TypeError):
        logger.warning(f"JSONè§£æå¤±è´¥: {data}")
        return default

def get_platform_icon(platform: str) -> str:
    """
    è·å–å¹³å°å›¾æ ‡
    """
    from config.platform_config import PLATFORM_CONFIG
    return PLATFORM_CONFIG.get(platform, {}).get('icon', 'ğŸ“±')

def get_platform_name(platform: str) -> str:
    """
    è·å–å¹³å°åç§°
    """
    from config.platform_config import PLATFORM_CONFIG
    return PLATFORM_CONFIG.get(platform, {}).get('name', platform)

def safe_parse_datetime(date_value: Any) -> Optional[datetime]:
    """
    å®‰å…¨åœ°è§£ææ—¥æœŸæ—¶é—´ï¼Œæ”¯æŒå­—ç¬¦ä¸²å’Œdatetimeå¯¹è±¡
    
    Args:
        date_value: å¾…è§£æçš„æ—¥æœŸæ—¶é—´ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–datetimeå¯¹è±¡ï¼‰
        
    Returns:
        datetimeå¯¹è±¡æˆ–Noneï¼ˆè§£æå¤±è´¥æ—¶ï¼‰
    """
    # å¦‚æœå·²ç»æ˜¯datetimeå¯¹è±¡ï¼Œç›´æ¥è¿”å›
    if isinstance(date_value, datetime):
        return date_value
        
    # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
    if isinstance(date_value, str):
        try:
            return datetime.fromisoformat(date_value)
        except ValueError:
            # å°è¯•å…¶ä»–å¸¸è§æ ¼å¼
            for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S.%fZ', '%Y-%m-%d']:
                try:
                    return datetime.strptime(date_value, fmt)
                except ValueError:
                    continue
                    
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸå­—ç¬¦ä¸²: {date_value}")
        return None
        
    # å…¶ä»–ç±»å‹
    logger.warning(f"ä¸æ”¯æŒçš„æ—¥æœŸç±»å‹: {type(date_value)}")
    return None
def process_tags(tags, max_length=100):
    """
    å¤„ç†æ ‡ç­¾åˆ—è¡¨ï¼Œç¡®ä¿æ¯ä¸ªæ ‡ç­¾é•¿åº¦ä¸è¶…è¿‡max_length
    
    Args:
        tags: æ ‡ç­¾åˆ—è¡¨æˆ–å•ä¸ªæ ‡ç­¾å­—ç¬¦ä¸²
        max_length: æ ‡ç­¾æœ€å¤§é•¿åº¦é™åˆ¶
        
    Returns:
        å¤„ç†åçš„æ ‡ç­¾åˆ—è¡¨
    """
    # ç¡®ä¿è¾“å…¥æ˜¯åˆ—è¡¨
    if not isinstance(tags, list):
        if not tags:  # ç©ºå€¼å¤„ç†
            return []
        tags = [str(tags)]  # è½¬æ¢ä¸ºå•å…ƒç´ åˆ—è¡¨
    
    processed = []
    for tag in tags:
        if isinstance(tag, str):
            # æˆªæ–­è¿‡é•¿æ ‡ç­¾
            if len(tag) > max_length:
                tag = tag[:max_length]
            if tag.strip():  # åªä¿ç•™éç©ºæ ‡ç­¾
                processed.append(tag.strip())
    return processed
def parse_json_string(s: str) -> Optional[List[Dict]]:
    """å°†JSONå­—ç¬¦ä¸²è§£æä¸ºæ•°ç»„ï¼ˆå¤„ç†å¯èƒ½çš„æ ¼å¼é”™è¯¯ï¼‰"""
    try:
        return json.loads(s)
    except json.JSONDecodeError as e:
        logger.warning(f"JSONå­—ç¬¦ä¸²è§£æå¤±è´¥: {e}, åŸå§‹å­—ç¬¦ä¸²: {s[:100]}")
        return None
    
def safe_fromisoformat(date_str: Any) -> Optional[datetime]:
    """
    å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetimeå¯¹è±¡
    
    Args:
        date_str: å¾…è½¬æ¢çš„æ—¥æœŸå­—ç¬¦ä¸²
        
    Returns:
        datetimeå¯¹è±¡æˆ–Noneï¼ˆè½¬æ¢å¤±è´¥æ—¶ï¼‰
    """
    if not isinstance(date_str, str):
        logger.warning(f"å°è¯•å°†éå­—ç¬¦ä¸²ç±»å‹è½¬æ¢ä¸ºdatetime: {type(date_str)}")
        return None
        
    try:
        return datetime.fromisoformat(date_str)
    except ValueError:
        # å°è¯•å¤„ç†å…¶ä»–å¸¸è§æ ¼å¼
        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S.%fZ', '%Y-%m-%d']:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
                
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸå­—ç¬¦ä¸²: {date_str}")
        return None
    
def safe_get(data, key, default=None):
    """å®‰å…¨è·å–æ•°æ®ï¼ˆæ”¯æŒåµŒå¥—å­—å…¸/åˆ—è¡¨ï¼‰"""
    if isinstance(data, dict):
        return data.get(key, default)
    elif isinstance(data, list) and isinstance(key, int) and 0 <= key < len(data):
        return data[key]
    return default

    # def scrape_platform_via_web(self, platform_code: str) -> List[Dict]:
    #     """ä¼˜åŒ–ç½‘é¡µè§£æï¼ˆä½œä¸ºAPIå¤‡ä»½ï¼‰"""
    #     config = self.get_platform_config(platform_code)
    #     if not config:
    #         return []
    #     tab = platform_code  # ç®€åŒ–tabæ˜ å°„
    #     url = f"{self.base_url}/?tab={tab}"
    #     try:
    #         html = self.fetch_page(url)
    #         if not html:
    #             return []
    #         # å°è¯•è§£æç½‘é¡µä¸­çš„JSONæ•°æ®ï¼ˆç°ä»£ç½‘ç«™å¸¸ç”¨ï¼‰
    #         soup = BeautifulSoup(html, 'html.parser')
    #         # æŸ¥æ‰¾å†…åµŒJSONï¼ˆå¦‚<script id="__NEXT_DATA__">ï¼‰
    #         script = soup.find('script', id='__NEXT_DATA__')
    #         if script:
    #             try:
    #                 web_json = json.loads(script.text)
    #                 web_items = safe_get(web_json, ['props', 'pageProps', 'data', 'list'])
    #                 if isinstance(web_items, list):
    #                     return self._parse_web_items(web_items, platform_code)
    #             except json.JSONDecodeError as e:
    #                 logger.warning(f"å¹³å° {platform_code} ç½‘é¡µJSONè§£æå¤±è´¥: {e}")
    #         return self._parse_web_html(soup, platform_code)
    #     except Exception as e:
    #         logger.error(f"å¹³å° {platform_code} ç½‘é¡µè§£æå¤±è´¥: {e}")
    #         return []

    # def _parse_web_items(self, items: List[Dict], platform_code: str) -> List[Dict]:
    #     """è§£æç½‘é¡µä¸­çš„JSONæ•°æ®é¡¹"""
    #     topics = []
    #     for idx, item in enumerate(items, 1):
    #         title = clean_text(safe_get(item, 'title', ""))
    #         if not title:
    #             continue
    #         topics.append({
    #             "platform": platform_code,
    #             "title": title,
    #             "rank": idx,
    #             "heat_value": self.extract_heat_value(safe_get(item, 'heat_str', "")),
    #             "timestamp": datetime.now().isoformat(),
    #             "hash_id": generate_hash(f"{title}_{platform_code}"),
    #             "tags": process_tags(clean_text(safe_get(item, 'tag', "")))  # æ·»åŠ è¿™è¡Œ
    #         })
    #     return topics

    # def _parse_web_html(self, soup: BeautifulSoup, platform_code: str) -> List[Dict]:
    #     """è§£æç½‘é¡µHTMLæ ‡ç­¾ï¼ˆæŒ‰å¹³å°å®šåˆ¶é€‰æ‹©å™¨ï¼‰"""
    #     # ä¸åŒå¹³å°ç½‘é¡µç»“æ„ä¸åŒï¼Œè¿™é‡Œä»¥æŠ–éŸ³ä¸ºä¾‹
    #     selectors = {
    #         'douyin': '.hot-item',
    #         'weibo': '.weibo-item',
    #         'zhihu': '.zhihu-topic'
    #     }
    #     selector = selectors.get(platform_code, '.hot-item')
    #     items = soup.select(selector)
    #     topics = []
    #     for idx, item in enumerate(items, 1):
    #         title_elem = item.select_one('.title')
    #         if not title_elem:
    #             continue
    #         title = clean_text(title_elem.text)
    #         heat_elem = item.select_one('.heat')
    #         heat_str = heat_elem.text if heat_elem else ""
    #         topics.append({
    #             "platform": platform_code,
    #             "title": title,
    #             "rank": idx,
    #             "heat_value": self.extract_heat_value(heat_str),
    #             "hash_id": generate_hash(f"{title}_{platform_code}")
    #         })
    #     return topics

    # def fetch_page(self, url: str, max_retries: int = 3) -> Optional[str]:
    #     """è·å–ç½‘é¡µå†…å®¹"""
    #     retries = 0
    #     while retries < max_retries:
    #         try:
    #             self.update_headers()
    #             response = self.session.get(url, timeout=10)
    #             response.raise_for_status()
    #             return response.text
    #         except RequestException as e:
    #             logger.warning(f"ç½‘é¡µ {url} è·å–å¤±è´¥ ({retries+1}/{max_retries}): {e}")
    #             retries += 1
    #             time.sleep(2 * retries)
    #     return None